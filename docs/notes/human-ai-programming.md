# 目标图景：人与 AI 编程的核心区别

> 一次架构重构中的实践观察与反思
>
> 2026-02-22 / wp-reactor CEP 引擎重构

---

## 背景

在 wp-reactor 的开发过程中，系统采用增量方式逐步构建：

1. **WindowBuffer** — 解决"数据怎么按时间存、怎么淘汰"
2. **CepStateMachine** — 解决"规则逻辑怎么跑"
3. **Receiver** — 解决"数据怎么进来"
4. **Scheduler + channel** — 解决"怎么让它们跑通"

每一步都在解决当时最紧迫的问题，channel 直连是让系统跑通的最短路径。系统编译通过，测试全绿，功能正确。

但在一次人工 code review 中，发现了一个根本性的架构问题：

```
实际数据流（错误）:
  Receiver → event_tx channel → Scheduler → CEP（处理）
  Receiver → Router → Window（存储，但没有人读取）

应有数据流（正确）:
  Receiver → Router → Window(+Notify) → CEP Engine tasks → Alert sink
```

CEP 引擎完全绕过了 Window 系统。Window 的 watermark、late policy、eviction 对 CEP 处理没有任何影响。

---

## 对话记录

### 一、为什么原来的设计是错的？

**问：** 为什么这个关键的设计，原来都是错的呢？

不是"错"，是增量开发的自然结果。channel 直连是让系统跑通的最短路径。跑通之后才能看清楚：Window 和 CEP 之间缺一层正确的关系。

这是正常的过程——先做出能工作的版本，在使用中发现组件之间的正确关系，再重构。如果一开始就设计游标 + Notify 模型，大概率会过度设计，因为当时还不知道增量和集合两种场景都需要。

### 二、自动化测试无法发现架构问题

**问：** 但，如果我不 review 代码，这样的错误就没有办法暴露解决。

对。这个问题不会通过测试暴露——所有测试都能通过，e2e 测试也能产出正确结果。数据从 channel 直达 CEP，绕过 Window，功能上没有 bug。

它是架构层面的问题，只有在以下场景才会暴露：

- 想用 watermark / late policy 过滤迟到数据 → 发现 CEP 根本不经过 Window，配置不生效
- 想做集合计算 → 发现 CEP 拿不到窗口快照
- 想让 `over` 控制 CEP 可见范围 → 发现 `over` 只影响 buffer 存储，CEP 无感知

这类问题只有人去审视"组件之间的关系是否合理"才能发现。自动化测试验证的是**行为正确性**，不验证**架构合理性**。

### 三、AI 编程的能力边界

**问：** 没有人类工程师，自动化编程能走多远呢？未来的 AI 可以解决这些问题么？

现阶段 AI 编码的能力边界很清晰：

**能做好的：** 给定明确规格的实现——函数实现、重构、模式套用、测试生成。

**做不好的：** 质疑架构的合理性。发现 Window 和 CEP 之间的关系不对，不是因为代码有 bug，而是因为理解 CEP 引擎在这个领域里**应该**怎么工作。这需要：

- 对领域的真实理解（不是模式匹配，是知道"窗口语义必须对计算可见"）
- 对当前设计的怀疑能力（代码能跑、测试通过，仍然追问"这对吗"）
- 跨组件的整体判断（不是看单个模块，是看数据流是否经过了正确的层次）

核心不是智能水平，而是**谁来定义"正确"**。重构 `start()` 函数——正确性标准明确，AI 可以自主完成。但"CEP 应该从 Window 读数据"——这个判断来自对系统**应该怎样工作**的认知，不是从代码中能推导出来的。

> 代码告诉你系统**实际**怎样工作，不告诉你**应该**怎样工作。这是两个根本不同的问题。

### 四、AI 缺少的是"不断正确的图像"

**问：** 这主要是因为 AI 对于这个系统没有最终的想象？不断正确的"图像"?

对。人脑中有一个 CEP 引擎完整运行时的画面——数据进来、经过窗口过滤、被规则消费、产出告警。这个画面不是从代码里读出来的，是从领域经验中构建的。每次 review 代码，是在拿实现去**对照**这个画面，发现不吻合就追问。

AI 没有这个画面。AI 看到的是：

- `event_tx` channel 存在 → 合理，组件间通信
- `router.route()` 存在 → 合理，数据写入 Window
- CEP 从 channel 读 → 合理，数据流通了
- 测试通过 → 没问题

每一步局部都合理，但 AI 不会主动问"这两条数据路径之间的关系对吗"，因为没有那个**应该是什么样**的参照。

"不断正确的图像"不是一次性的设计文档，而是随着系统演进持续更新的。这个图像指导判断什么是对的、什么该质疑、下一步该往哪走。

### 五、核心区别

**问：** 这个目标的图景，才是人与 AI 的核心区别。

技能层面的差距会不断缩小——写代码、重构、调试、甚至设计模式的运用，AI 都会越来越好。但"这个系统应该长什么样"这个问题，性质不同。

它不是推理问题，不是从现有代码或文档中能推导出来的。它来自对领域的理解、对用户需求的判断、对技术取舍的直觉，以及不断在实践中修正的过程。

> 工具可以越来越强，但决定**用工具造什么**的，是持有图景的人。

---

## 实践验证

这次对话之后，完成了完整的 9 步架构重构：

| 步骤 | 内容 | 执行者 |
|------|------|--------|
| 架构问题发现 | 识别 Window 与 CEP 的断裂 | 人 |
| 目标架构设计 | 游标 + Notify 模型 | 人 |
| 9 步实施计划 | 拆分为可编译可测试的增量步骤 | 人 |
| Step 1-8 代码实现 | 修改 12 个文件，删除 scheduler，新建 engine_task | AI |
| Step 9 调试 | Notify 竞态、两阶段关闭、watermark 时序 bug | AI |
| 验证 | 393 项测试全部通过，clippy 零警告 | AI |

过程中 AI 遇到的 watermark bug 尤其能说明问题：`append_with_watermark()` 先推进水位再做延迟检查，导致一个批次内的早期事件被自身推进的水位判定为迟到。AI 能高效定位并修复这个 bug——但它是在人定义的架构框架内做局部推理，而非质疑框架本身。

---

## 结论

```
人的工作：  持有并演进"系统应该是什么样"的图景
AI 的工作：  在图景确定后，高效、准确地实现它

行为正确性 → 可自动化验证
架构合理性 → 需要人的判断
```

两者是协作关系，不是替代关系。人负责方向，AI 负责速度。
